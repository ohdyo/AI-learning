{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword Tokenizer\n",
    "- 특정 도메인에 특화된 데이터 셋팅 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
      "Downloading data from https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\ljh10/.torch/datasets\\\\ratings_train.txt',\n",
       " 'C:\\\\Users\\\\ljh10/.torch/datasets\\\\ratings_test.txt')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 네이버 영화 리뷰 데이터\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "def get_file(filename, origin):\n",
    "    cache_dir = os.path.expanduser('~/.torch/datasets')\n",
    "    os.makedirs(cache_dir,exist_ok=True)\n",
    "    filepath = os.path.join(cache_dir, filename)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f'Downloading data from {origin}')\n",
    "        urllib.request.urlretrieve(origin,filepath)\n",
    "\n",
    "    return filepath\n",
    "\n",
    "ratings_train_path = get_file(\"ratings_train.txt\", \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\")\n",
    "ratings_test_path = get_file(\"ratings_test.txt\", \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\")\n",
    "ratings_train_path,ratings_test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratings_train_df = pd.read_csv(ratings_train_path, sep='\\t')\n",
    "ratings_test_df = pd.read_csv(ratings_test_path, sep='\\t')\n",
    "\n",
    "display(ratings_train_df)\n",
    "display(ratings_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((149995, 3), (49997, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측치 제거\n",
    "ratings_train_df = ratings_train_df.dropna(how='any')\n",
    "ratings_test_df = ratings_test_df.dropna(how='any') # how의 인자로 any를 넘기면 행 열 아무거나 비어있어도 드랍함\n",
    "\n",
    "ratings_train_df.shape, ratings_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt 팡리 생성 - 학습 데이터\n",
    "with open('naver_review.txt', 'w', encoding='utf-8') as f:\n",
    "    for doc in ratings_train_df['document'].values:\n",
    "        f.write(doc+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentencePieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl.metadata (8.3 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/992.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 992.0/992.0 kB 11.8 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "input = 'naver_review.txt'\n",
    "vocab_size = 10000 \n",
    "model_prefix = 'naver_review'\n",
    "cmd = f'--input={input} --model_prefix={model_prefix} --vocab_size={vocab_size}'\n",
    "\n",
    "spm.SentencePieceTrainer.Train(cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 더빙.. 진짜 짜증나네요 목소리\n",
      "['▁아', '▁더빙', '..', '▁진짜', '▁짜증나', '네요', '▁목소리']\n",
      "[62, 877, 5, 31, 2019, 68, 1710]\n",
      "\n",
      "흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
      "['▁흠', '...', '포스터', '보고', '▁초딩', '영화', '줄', '....', '오', '버', '연기', '조차', '▁가볍지', '▁않', '구나']\n",
      "[1634, 8, 4908, 159, 1460, 33, 264, 60, 173, 548, 410, 1224, 7396, 754, 440]\n",
      "\n",
      "너무재밓었다그래서보는것을추천한다\n",
      "['▁너무', '재', '밓', '었다', '그래서', '보', '는것을', '추천', '한다']\n",
      "[23, 369, 9781, 429, 3780, 143, 6266, 1945, 314]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(f'{model_prefix}.model')\n",
    "\n",
    "for doc in ratings_train_df['document'].values[:3]:\n",
    "    print(doc)\n",
    "    print(sp.encode_as_pieces(doc))\n",
    "    print(sp.encode_as_ids(doc))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "걸작은 몇안되고 졸작들만 넘쳐난다.\n",
      "['▁걸작', '은', '▁몇', '안되고', '▁졸작', '들만', '▁넘', '쳐', '난다', '.']\n",
      "[1060, 18, 621, 6979, 728, 3291, 165, 705, 1003, 4]\n",
      "걸작은 몇안되고 졸작들만 넘쳐난다.\n",
      "걸작은 몇안되고 졸작들만 넘쳐난다.\n",
      "걸작은 몇안되고 졸작들만 넘쳐난다.\n"
     ]
    }
   ],
   "source": [
    "text = ratings_test_df['document'][100]\n",
    "tokens = sp.encode_as_pieces(text)\n",
    "id_tokens = sp.encode_as_ids(text)\n",
    "print(text)\n",
    "print(tokens)\n",
    "print(id_tokens)\n",
    "\n",
    "print(''.join(tokens).replace('▁', \" \").strip())\n",
    "\n",
    "print(sp.decode_pieces(tokens))\n",
    "print(sp.decode_ids(id_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# lower_case : 대소문자 구분 인자\n",
    "# strip_accents : 악센트 제거 인자\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=False, strip_accents=False)\n",
    "vocab_size = 10000\n",
    "\n",
    "tokenizer.train(\n",
    "    files=['naver_review.txt'],\n",
    "    vocab_size = vocab_size, # vocab 사이즈\n",
    "    min_frequency = 5, # 최소 빈도수 2번 이상 나온 단어만 사용\n",
    "    show_progress=True # 학습 진행 상황을 보여줌\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아버지가', '방', '##에', '들어가', '##신', '##다', '.']\n",
      "[8317, 482, 1032, 6765, 1215, 1027, 16]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode('아버지가 방에 들어가신다.') # 토큰화된 결과를 반환\n",
    "print(encoded.tokens) # 토큰화된 결과\n",
    "print(encoded.ids)  # 토큰에 대응하는 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아버지가 방에 들어가신다.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = tokenizer.decode(encoded.ids) # 토큰화된 인덱스를 텍스트로 반환\n",
    "decoded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
