{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰화 (Tokenization)\n",
    "**토큰화 목적**\n",
    "- 문법적 구조 이해\n",
    "- 유연한 데이터 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'NLP is fascinationg. It has many applications in real-world scenarios.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'is', 'fascinationg', '.', 'It', 'has', 'many', 'applications', 'in', 'real-world', 'scenarios', '.']\n",
      "['NLP is fascinationg.', 'It has many applications in real-world scenarios.']\n",
      "['NLP', 'is', 'fascinationg', '.']\n",
      "['It', 'has', 'many', 'applications', 'in', 'real-world', 'scenarios', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# 단어 토큰화\n",
    "words = nltk.word_tokenize(text)\n",
    "print(words)\n",
    "\n",
    "# 문장 토큰화\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(sentences)\n",
    "\n",
    "# 문장별 단어 토큰화\n",
    "for sent in nltk.sent_tokenize(text):\n",
    "    print(nltk.word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BertTokenizer\n",
    "    - 단어를 부분 단위로 쪼개어 희귀하거나 새로운 단어도 부분적으로 표현할 수 있도록 함 -> 어휘 크기를 줄이고 다양한 언어 패턴 학습 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['un', '##ha', '##pp', '##iness']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "word = 'unhappiness'\n",
    "subwords = tokenizer.tokenize(word)\n",
    "subwords\n",
    "# 일반적인 토큰화와 달리 단어를 서브워드로 분리\n",
    "# 분리가능한 가장 작은 단위\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분리과정\n",
    "1. WordPiece 토크나이저는 사전에 등록된 단어 우선 사용\n",
    "    - Bert의 WordPiece 토크나이저는 최대한 긴 단어를 먼저 찾으려는 방식으로 동작\n",
    "    - 'happy'가 사전에 있으면 그대로 사용, 없으면 가장 긴 서브워드 조합 찾아 나눔\n",
    "2. 'happy'가 사전에 포함되지 않은 경우\n",
    "    - 일반적으로 'happy'는 BERT의 사전에 포함되어 있지만, 학습된 모델에 없을 수 있음\n",
    "    - 'happy'가 사전에 없는 경우 사전에 있는 조각(서브워드)로 분해\n",
    "3. 그래서 과정은...\n",
    "    1. \"unhappiness\" 검색 -> 없음\n",
    "    2. \"un\" 검색 -> 있음 -> 유지\n",
    "    3. \"happiness\" 검색 -> 없음\n",
    "    4. \"ha\" 검색 -> 있음\n",
    "    5. \"pp\" 검색 -> 있음\n",
    "    6. \"iness\" 검색 -> 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nl',\n",
       " '##p',\n",
       " 'is',\n",
       " 'fascination',\n",
       " '##g',\n",
       " '.',\n",
       " 'it',\n",
       " 'has',\n",
       " 'many',\n",
       " 'applications',\n",
       " 'in',\n",
       " 'real',\n",
       " '-',\n",
       " 'world',\n",
       " 'scenarios',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subwords = tokenizer.tokenize(text)\n",
    "subwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문자 단위 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u', 'n', 'h', 'a', 'p', 'p', 'i', 'n', 'e', 's', 's']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'unhappiness'\n",
    "list(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화 주의사항\n",
    "1. 구두점이나 특수 문자를 단순 제외하면 안됨\n",
    "2. 줄임말, 단어 내 띄어쓰기 유의\n",
    "\n",
    "**표준 토큰화 예제 *Penn Treebank Tokenization**\n",
    "- 규칙 1: 하이픈으로 구성된 단어는 하나로 유지한다.\n",
    "- 규칙 2: doesn't와 같이 '가 있는 단어는 분리한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time',\n",
       " 'flies',\n",
       " 'like',\n",
       " 'an',\n",
       " 'arrow',\n",
       " 'fruit',\n",
       " 'flies',\n",
       " 'like',\n",
       " 'a',\n",
       " 'banana']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 구두점을 제외한 단어 토큰화\n",
    "import re\n",
    "text = \"Time flies like an arrow; fruit flies like a banana.\"\n",
    "re.findall(r'\\b\\w+\\b', text)    \n",
    "# \\b : 단어 경계를 의미하는 메타 문자 (boundary / 공백, 구두점, ..)\n",
    "# \\w : 단어 문자를 의미하는 메타 문자 (word)\n",
    "# \\w+ : 단어 문자가 1개 이상인 경우를 의미하는 정규 표현식\n",
    "# r : 파이썬 roaw text, raw string을 의미하는 접두사 (이스케이핑 문자를 문자 그대로 처리)\n",
    "# => : 경계 문자로 감싸진 하나 이상의 글자를 전부 찾아라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'hesitate', 'to', 'use', 'well', '-', 'being', 'practices', 'for', 'self', '-', 'care', '.']\n",
      "['Do', \"n't\", 'hesitate', 'to', 'use', 'well-being', 'practices', 'for', 'self-care', '.']\n"
     ]
    }
   ],
   "source": [
    "# WordPunctTokenizer\n",
    "# 단어/구두점으로 토큰을 구분(',- 포함 단어도 분리)\n",
    "from nltk.tokenize import WordPunctTokenizer, word_tokenize\n",
    "\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "\n",
    "text = \"Don't hesitate to use well-being practices for self-care.\"\n",
    "print(word_punct_tokenizer.tokenize(text))\n",
    "\n",
    "print(word_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID-19', '(', '전염병', ')', ',', 'Dr.Smith', '(', '의사', ')', ',', 'NASA', '(', '우주항공국', ')', '등', '특정', '기관이나', '명칭이', '있다.', '특수', '문자', '또한', '태그', '<', 'br', '>', ',', '가격', '$', '100.50', ',', '2025/02/18', '날짜', '표현에', '사용될', '수', '있다.', '이러한', '경우', ',', '$', '100.50을', '하나의', '토큰으로', '유지할', '필요가', '있다', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "treebank_word_tokenizer = TreebankWordTokenizer()\n",
    "text = '''\n",
    "COVID-19(전염병), Dr.Smith(의사), NASA(우주항공국) 등 특정 기관이나 명칭이 있다. \\\n",
    "특수 문자 또한 태그 <br>, 가격 $100.50, 2025/02/18 날짜 표현에 사용될 수 있다. \\\n",
    "이러한 경우, $100.50을 하나의 토큰으로 유지할 필요가 있다.\n",
    "'''\n",
    "print(treebank_word_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kss==5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
      "For your information, Kss also supports mecab backend.\n",
      "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
      "Please refer to following web sites for details:\n",
      "- mecab: https://cleancode-ws.tistory.com/97\n",
      "- konlpy.tag.Mecab: https://uwgdqo.tistory.com/363\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['배경은 1920년대의 경성부이다.', '주인공이자 인력거꾼 김 첨지의 아내는 병에 걸린 지 1달 가량이 지나 있었다.', \"아내는 단 한 번도 약을 먹어본 적이 없는데, 그 이유는 '병이란 놈에게 약을 주어 보내면 재미를 붙여서 자꾸 온다.'는 김 첨지의 신조 때문.\", '멍청이, 꼰대...가 아닐수 없다.', '사실 이건 핑계고, 약을 살 돈도 벌지 못하고 있었다는 이유가 더 크다.']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "text = \"배경은 1920년대의 경성부이다. 주인공이자 인력거꾼 김 첨지의 아내는 병에 걸린 지 1달 가량이 지나 있었다. 아내는 단 한 번도 약을 먹어본 적이 없는데, 그 이유는 '병이란 놈에게 약을 주어 보내면 재미를 붙여서 자꾸 온다.'는 김 첨지의 신조 때문. 멍청이, 꼰대...가 아닐수 없다. 사실 이건 핑계고, 약을 살 돈도 벌지 못하고 있었다는 이유가 더 크다.\"\n",
    "\n",
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ljh10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\ljh10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Time', 'NNP'),\n",
       " ('flies', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('arrow', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = \"Time flies like an arrow.\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.cli.download('en_core_web_sm')    # 직접 다운로드 (사용전 1회 반드시 다운을 실행해야함)\n",
    "spacy_nlp = spacy.load('en_core_web_sm')# 로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time : NOUN\n",
      "flies : VERB\n",
      "like : ADP\n",
      "an : DET\n",
      "arrow : NOUN\n",
      ". : PUNCT\n"
     ]
    }
   ],
   "source": [
    "tokens = spacy_nlp(text)\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, \":\", token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoNLPy\n",
    "- 한국어 자연어 처리를 위한 라이브러리\n",
    "- 형태소 분석, 품사 태깅, 텍스트 전처리 등 기능 지원\n",
    "- 여러 형태소 분석기 중 적합한 분석기 선택 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting JPype1>=0.7.0 (from konlpy)\n",
      "  Downloading jpype1-1.5.2-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\ljh10\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from konlpy) (5.3.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\ljh10\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from konlpy) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\ljh10\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (24.2)\n",
      "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "   ---------------------------------------- 0.0/19.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 2.4/19.4 MB 12.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 4.7/19.4 MB 11.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 7.1/19.4 MB 11.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 9.7/19.4 MB 11.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 12.1/19.4 MB 11.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 14.4/19.4 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 16.8/19.4 MB 11.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 18.9/19.4 MB 11.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 19.4/19.4 MB 11.4 MB/s eta 0:00:00\n",
      "Downloading jpype1-1.5.2-cp312-cp312-win_amd64.whl (356 kB)\n",
      "Installing collected packages: JPype1, konlpy\n",
      "Successfully installed JPype1-1.5.2 konlpy-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '는', '버스', '를', '기다리고', '있다', '.', '지각', '은', '면', '하겠군', '!']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "text = \"나는 버스를 기다리고 있다. 지각은 면하겠군!\"\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "# 형태소 분석석\n",
    "morphs = okt.morphs(text)\n",
    "morphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('나', 'Noun'),\n",
       " ('는', 'Josa'),\n",
       " ('버스', 'Noun'),\n",
       " ('를', 'Josa'),\n",
       " ('기다리고', 'Verb'),\n",
       " ('있다', 'Adjective'),\n",
       " ('.', 'Punctuation'),\n",
       " ('지각', 'Noun'),\n",
       " ('은', 'Josa'),\n",
       " ('면', 'Noun'),\n",
       " ('하겠군', 'Verb'),\n",
       " ('!', 'Punctuation')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 품사 태깅\n",
    "pos_tags = okt.pos(text)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '버스', '지각', '면']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 명사 태깅\n",
    "nouns = okt.nouns(text)\n",
    "nouns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
