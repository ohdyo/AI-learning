# ë¶„ë¥˜ (Classification)
- ì…ë ¥ ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ì •ì™¸ëœ ì—¬ëŸ¬ í´ë˜ìŠ¤ ì¤‘ í•˜ë‚˜ë¡œ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ(ë²”ì£¼í˜• ë°ì´í„°)
    - ì´ì§„ ë¶„ë¥˜ : ì–‘ì„±(1), ìŒì„±(0) ì¤‘ì— í•˜ë‚˜ë¥¼ ë§ì¶”ëŠ” ê²ƒ
    - ë‹¤ì¤‘ ë¶„ë¥˜ : ì—¬ëŸ¬ í´ë˜ìŠ¤ ì¤‘ í•˜ë‚˜ë¥¼ ë§ì¶”ëŠ” ê²ƒ

##  Logistic Regression
- ì„ í˜• íšŒê·€ ë°©ì‹ìœ¼ë¡œ ë¶„ë¥˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ëª¨ë¸
    - ì´ì§„ ë¶„ë¥˜ : ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜(ì‹œê·¸ëª¨ì´ë“œ)ë¥¼ í†µí•´ í™•ë¥ ê°’ì„ ê³„ì‚°í•˜ê³  0 ë˜ëŠ” 1ë¡œ ë¶„ë¥˜ 
    - ë‹¤ì¤‘ ë¶„ë¥˜ : ë‹¤ì¤‘ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ í†µí•´ ê° í´ë˜ìŠ¤ë³„ í™•ë¥ ê°’ì„ ê³„ì‚°í•´ ë‹¤ì¤‘ ë¶„ë¥˜
### ***ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ Sigmoid í•¨ìˆ˜ + Logistic Regression ì´ìš©***
- **ì„ í˜•íšŒê·€ì‹ì„ í†µí•´ ë„ì¶œí•œ ì˜ˆì¸¡ê°’(z)ì„ 0ê³¼ 1 ì‚¬ì´ì˜ ìˆ˜ë¡œ ë³€í™˜í•´**ì£¼ëŠ” í™œì„±í™” í•¨ìˆ˜(Activation Function)
$
    ì‹œê·¸ëª¨ì´ë“œ(z) = \frac{1}{1+e^{-z}}
$
    <code>sigmoid_value = 1 /(1 + np.exp(-z)) # np.exp(-z) = e^-z</code>

    - ì‹œê·¸ëª¨ì´ë“œì˜ ê°’ì€ ***zê°’ì˜ í¬ê¸°ì™€ ë°˜ë¹„ë¡€*** í•œë‹¤.
```python
# z = ì„ í˜•íšŒê·€ ê²°ê³¼ ëª¨ë¸
# ì‹œê·¸ëª¨ì´ë“œ ì‹œê°í™”
z = np.linspace(-5,5,100) # ì„ í˜•íšŒê·€ ê²°ê³¼ê°’
sigmoid_value = 1 /(1 + np.exp(-z)) # np.exp(-z) = e^-z

plt.plot(z, sigmoid_value)
plt.xlabel('Z')
plt.ylabel('sigmoid(z)')
plt.grid()
plt.show()
```
- ***ë¡œì§€ìŠ¤í‹± ë¶„ë¥˜ êµ¬í˜„***

```python
# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
fish_df = pd.read_csv('./data/fish.csv')
is_bream_orsmelt = (fish_df['Species'] == 'Bream') | (fish_df['Species'] == 'Smelt')
fish_df = fish_df[is_bream_orsmelt]

# í›ˆë ¨ë°ì´í„° - í…ŒìŠ¤íŠ¸ë°ì´í„° ì…‹íŒ… ë° ì •ê·œí™”
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X = fish_df.drop('Species', axis=1)
y = fish_df['Species']

X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.fit_transform(X_test)

# ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ê°€
from sklearn.linear_model import LogisticRegression

lr_clf = LogisticRegression()
lr_clf.fit(X_train_scaled, y_train)

lr_clf.score(X_train_scaled, y_train), lr_clf.score(X_test_scaled, y_test)  
# (1.0, 1.0)

# ì˜ˆì¸¡ê°’ë“¤ì„ í†µí•œ ë¶„ì„
y_pred = lr_clf.predict(X_test_scaled[:3])
y_pred # 'Bream', 'Smelt', 'Smelt'

print(lr_clf.classes_) # ['Bream' 'Smelt']
lr_clf.predict_proba(X_test_scaled[:3])
#array([[0.96120317, 0.03879683],
    #    [0.00842591, 0.99157409],
    #    [0.01439468, 0.98560532]])

# ì„ í˜•íšŒê·€ê°’ ì§ì ‘ ê³„ì‚°
z1 = np.dot(X_test_scaled[:3], lr_clf.coef_[0]) + lr_clf.intercept_
# ì„ í˜• íšŒê·€ê°’ ê³„ì‚° í•¨ìˆ˜ (decision_function)
z2 = lr_clf.decision_function(X_test_scaled[:3])
# (array([-3.20984727,  4.76798194,  4.22639728]),
#  array([-3.20984727,  4.76798194,  4.22639728]))

# ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ì ìš©
sigmoid_value = 1 / (1 + np.exp(-z1))
sigmoid_value # array([0.03879683, 0.99157409, 0.98560532])

# ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ì ìš©ë¬ëŠ”ì§€ í™•ì¸
['Smelt' if value >= 0.5 else 'Bream' for value in sigmoid_value] # ['Bream', 'Smelt', 'Smelt']
```


### ***ë‹¤ì¤‘ ë¶„ë¥˜ë¥¼ ìœ„í•œ Softmaxí•¨ìˆ˜ + logistic Regression***
- ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ë¥¼ ìœ„í•œ í™œì„±í™” í•¨ìˆ˜ë¡œ ê° í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ ê°’ ê³„ì‚°
- k ê°œì˜ í´ë˜ìŠ¤ê°€ ì¡´ì¬í•  ë•Œ ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°

$
    softmax(z_i) = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}
$
- $z_k$ : ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ì ìˆ˜ (ì…ë ¥ê°’)
- $e^{z_k}$ : í•´ë‹¹ ì ìˆ˜ì— ëŒ€í•œ ì§€ìˆ˜ í•¨ìˆ˜ ì ìš©
    - $\sum_{j=1}^{K} e^{z_j}$ : ëª¨ë“  í´ë˜ìŠ¤ ì ìˆ˜ì— ëŒ€í•´ ì§€ìˆ˜ í•¨ìˆ˜ ì ìš© í›„ ì´í•©
    - **ë‹¤ì¤‘ í´ë˜ìŠ¤ í™•ë¥  ê³„ì‚° ìˆœì„œ**
        1. ìƒ˜í”Œì— ëŒ€í•œ íšŒê·€ ê²°ê³  z ê³„ì‚°
        2. ì†Œí”„íŠ¸ ë§¥ìŠ¤ í•¨ìˆ˜ ì ìš©
            - zë¥¼ eì˜ ì§€ìˆ˜ë¡œ ì ìš©í•´ ê°’ì„ í™•ëŒ€(í´ë˜ìŠ¤ë³„ zì˜ ì°¨ì´ë¥¼ ê·¹ëŒ€í™”)
            - ***í•©ì„ ê° í´ë˜ìŠ¤ì˜ ê°’ìœ¼ë¡œ ë‚˜ëˆ  ë¹„ìœ¨ì„ ê³„ì‚°í•˜ê³  ë°˜í™˜***
        3. ê°€ì¥ ë†’ì€ í™•ë¥  ê°’ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ì„ íƒ
- ê²°ì • í•¨ìˆ˜ êµ¬í•˜ëŠ” ì½”ë“œ
<code>Z = lr_clf.decision_function(X_test[:5]) # ì„ í˜• íšŒê·€ê°’ ê³„ì‚°</code>

- ë‹¤ì¤‘ë¶„ë¥˜ë¥¼ ìœ„í•œ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ê°’ ì¶œë ¥ ë¼ì´ë¸ŒëŸ¬ë¦¬ **scipy**
<code>y_pred_proba = scipy.special.softmax(Z, axis=1)</code>
```python
# ë°ì´í„° ì…‹ ìƒì„±
from sklearn.datasets import make_classification

X,y = make_classification(    # ë¶„ë¥˜ ë¬¸ì œ ì—°ìŠµì„ ìœ„í•œ ê°€ìƒ ë°ì´í„°ì…‹ ìƒì„± í•¨ìˆ˜
    n_samples=100,      # ìƒ˜í”Œ ê°¯ìˆ˜
    n_features=4,       # ì „ì²´ íŠ¹ì„±(=ì»¬ëŸ¼) ê°œìˆ˜ 
    n_informative=3,    # ìœ ì˜ë¯¸í•œ íŠ¹ì„± ê°œìˆ˜
    n_redundant=0,      # ì¤‘ë³µ íŠ¹ì„± ê°œìˆ˜
    n_classes=3,        # í´ë˜ìŠ¤ ìˆ˜
    random_state=42     # ëœë¤ ì‹œë“œ
)
df = pd.DataFrame(X, columns=['feat1','feat2','feat3','feat4'])
df['target'] = y

# ë°ì´í„° ë¶„ë¦¬
X_train,X_test, y_train,y_test = train_test_split(X,y,random_state=42)

# predict_proba = í´ë˜ìŠ¤ë³„ 
lr_clf = LogisticRegression(max_iter=1000)
lr_clf.fit(X_train, y_train)
lr_clf.score(X_train,y_train), lr_clf.score(X_test,y_test)

y_pred = lr_clf.predict(X_test[:5])
y_pred_proba = lr_clf.predict_proba(X_test[:5])
y_pred_proba, y_pred_proba.sum(axis=1)

# ì§ì ‘ ê²Œì‚°
W = lr_clf.coef_
B = lr_clf.intercept_

W.shape, B.shape #((3,4) = (í´ë˜ìŠ¤ìˆ˜, íŠ¹ì„±ìˆ˜), (3,) = (í´ë˜ìŠ¤ìˆ˜))
# ê²°ì •í•¨ìˆ˜ (ì„ í˜•íšŒê·€ê°’ ê³„ì‚°)
Z = lr_clf.decision_function(X_test[:5])

# softmax í•¨ìˆ˜
def softmax(z):
    exp_z = np.exp(z)
    # sumì˜ í˜•ì‹ì„ ìœ ì§€í•´ì•¼ë§Œ ê°’ì„ ê³„ì‚° ê°€ëŠ¥í•˜ë‹¤.
    sum_exp_z = np.sum(exp_z, axis=1, keepdims=True)
    
    return exp_z / sum_exp_z

y_pred_proba = softmax(Z)
y_pred_proba # ì•„ë˜ì™€ ê°™ì€ ê°’ ë‚˜ì˜´

# scipyì˜ softmax í•¨ìˆ˜ (ë‹¤ì¤‘ë¶„ë¥˜ë¥¼ ìœ„í•œ softmaxí•¨ìˆ˜ë¥¼ êµ¬í˜„í•´ë‘” ë¼ì´ë¸ŒëŸ¬ë¦¬)
import scipy
import scipy.special

y_pred_proba = scipy.special.softmax(Z, axis=1)
y_pred_proba # ìœ„ë‘ ê°™ì€ ê°’ ë‚˜ì˜´ì˜´
```

***ìµœì¢…ì ìœ¼ë¡œ ìš°ë¦¬ê°€ êµ¬í•´ì•¼í• ê±´ Z(ì„ í˜• ëª¨ë¸ì˜ ê²°ê³¼ ê°’)ì˜ ê°’ ê³¼ ì´ë¥¼ í†µí•œ softmaxí•¨ìˆ˜ì˜ ì¸ìë¡œ ì‚¬ìš©í•˜ì—¬ì„œ í™•ë¥ ì„ êµ¬í•œë‹¤. ***

---

## ê²°ì • íŠ¸ë¦¬ - ë¶„ë¥˜
- ìŠ¤ë¬´ê³ ê°œ ì²˜ëŸ¼ ì§ˆë¬¸/ê²°ì •ì„ í†µí•´ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸
    - ***ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì˜í–¥ì´ ì ìŒ***
    - ì„ í˜• êµ¬ì¡°ê°€ ì•„ë‹Œ ë³µì¡í•œ êµ¬ì¡°ì˜ ë°ì´í„°ì— ì í•©
    - ê³¼ëŒ€ ì í•© ë˜ê¸° ì‰¬ì›€ -> ê°€ì§€ì¹˜ê¸° ë“±ì„ í†µí•´ ê³¼ëŒ€ì í•© ë°©ì§€
- graphviz ëª¨ë“ˆì„ ì„¤ì¹˜í•´ì•¼ í•˜ëŠ”ë° osì— ì§ì ‘ ì„¤ì¹˜í•´ì„œ Pathë¡œ ê²½ë¡œë¥¼ ì´ì–´ì¤˜ì•¼ í•œë‹¤. ê·¸ë¦¬ê³  íŒŒì´ì¬ ëª¨ë“ˆì— 'graphviz'ë¥¼ ì„¤ì¹˜í•˜ë©´ ì„í¬íŠ¸ë¥¼ í•˜ì§€ ì•Šì•„ë„ ìë™ìœ¼ë¡œ ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë‹¤.
### ì´ì§„ ë¶„ë¥˜
```python
# ë°ì´í„° ë¡œë“œ
wine_df = pd.read_csv('./data/wine_simple.csv')
wine_df

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ë°ì´í„° ë¶„ë¦¬ ë° ìŠ¤ì¼€ì¼ë§
X = wine_df.drop('class', axis=1)
y = wine_df['class']
y.value_counts()

X_train, X_test, y_train,y_test = train_test_split(X,y,random_state=0)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

#ëª¨ë“ˆ í•™ìŠµ
from sklearn.tree import DecisionTreeClassifier

dt_clf = DecisionTreeClassifier(random_state=0, max_depth=3)
dt_clf.fit(X_train, y_train)

dt_clf.score(X_train, y_train), dt_clf.score(X_test, y_test)

# íŠ¸ë¦¬ ì¶œë ¥
from sklearn.tree import plot_tree

plt.figure(figsize=(20,10))
plot_tree(
    dt_clf,
    filled=True,
    feature_names=X.columns,
    class_names=['red wine', 'white wine']
    )
plt.savefig('./images/wind_simple.png')
plt.show()

# íŠ¹ì„± ì¤‘ìš”ë„
# [alchole, suagr, pH] ì¤‘ìš”ë„
# - ì§€ë‹ˆë¶ˆìˆœë„ ê°ì†Œì— ê¸°ì—¬í•œ ë§Œí¼ ì¤‘ìš”ë„ê°€ ë†’ì•„ì§ì§
dt_clf.feature_importances_
```
- ëª¨ë“ˆ í•™ìŠµ ë¶€ë¶„
    - DecisionTreeClassifier()ì— ì‚¬ìš©ë˜ëŠ” ì¸ì
        - í•´ë‹¹ íŠ¸ë¦¬ì˜ ê¹Šì´ë¥¼ ì œí•œì‹œì¼œì¤Œ
- íŠ¸ë¦¬ ì¶œë ¥ ë¶€ë¶„
    - plt_tree()ë¡œ ì‚¬ìš©ë˜ëŠ” ì´ì
        1. dt_clf : ì‚¬ìš©ë˜ëŠ” í•™ìŠµëœ ëª¨ë¸
        2. filled=True : ìƒ‰ ì±„ìš°ê¸° ì˜µì…˜ (íŠ¹ì • í´ë˜ìŠ¤ì˜ ë¹„ìœ¨ í‘œí˜„)
        3. feature_name=X.columns : íŠ¹ì„± ì´ë¦„ì„ ìˆœì„œëŒ€ë¡œ ë§¤ì¹­ì‹œì¼œ ê°€ì ¸ì˜´
        4. calss_names=[] : í´ë˜ìŠ¤ ì´ë¦„, í•´ë‹¹ ë…¸ë“œ ì¤‘ ë” ë§ì€ ë°ì´í„°ê°€ ë“¤ì–´ê°„ í´ë˜ìŠ¤ì˜ ì´ë¦„ì„ ë³´ì—¬ì¤Œ
    - plt.savefig('Path')
        - ìƒì„±ëœ íŠ¸ë¦¬ë¥¼ í™•ì¥ì¥ìëª…ì— ë§ê²Œ ë³€í™˜í•´ì„œ ì €ì¥ì‹œì¼œì¤Œ

- íŠ¸ë¦¬ì— ë‹´ê¸´ ë°ì´í„° í™•ì¸
```plain text
ë£¨íŠ¸ ë…¸ë“œì˜ ì¶œë ¥ ë°ì´í„°
sugar <= 0.284      # DecisionTreeClassifierê°€ ì •í•œ ë¶„í• ê¸°ì¤€ (ìì‹ ë…¸ë“œì—ì„œ ì§€ë‹ˆê³„ìˆ˜ê°€ ìµœëŒ€ë¡œ ë‚®ì•„ì§ˆ ìˆ˜ ìˆëŠ” ë¶„í•  ê¸°ì¤€)
gini = 0.373    # ì§€ë‹ˆê³„ìˆ˜ = 1 - (ìŒì„±í´ë˜ìŠ¤ë¹„ìœ¨^2 + ì–‘ì„±í´ë˜ìŠ¤ë¹„ìœ¨^2)
samples = 4872  # í˜„ì¬ ë…¸ë“œì˜ ì „ì²´ ìƒ˜í”Œìˆ˜
value = [1207,3665] # í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ê°œìˆ˜ (0ë²ˆ í´ë˜ìŠ¤ 1207ê°œ, 1ë²ˆ í´ë˜ìŠ¤ 3665ê°œ)
class = white wine  # í˜„ì¬ ë…¸ë“œì˜ í´ë˜ìŠ¤ (valueì—ì„œ ë§ì€ í´ë˜ìŠ¤ ì„ íƒ)
```
- íŠ¸ë¦¬ ëª¨ë¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì£¼ìš” ë©”ì„œë“œ
    - feature_importances_
        - íŠ¹ì„± ì¤‘ìš”ë„
        - ì§€ë‹ˆ ë¶ˆìˆœë„ ê°ì†Œì— ê¸°ì—¬í•œ ë§Œí¼ ì¤‘ìš”ë„ê°€ ë†’ì•„ì§

### ë‹¤ì¤‘ ë¶„ë¥˜
```python

# ë°ì´í„° ë¡œë“œ ë° ë¶„ë¦¬
from sklearn.datasets import load_iris

iris_data = load_iris() # data: xë°ì´í„°, target : y ë°ì´í„°
X_train,X_test,y_train,y_test = train_test_split(iris_data.data,iris_data.target,random_state=0)

# ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
dt_clf = DecisionTreeClassifier(random_state=42, max_depth=3)
dt_clf.fit(X_train,y_train)

dt_clf.score(X_train,y_train), dt_clf.score(X_test, y_test)

# íŠ¸ë¦¼ëª¨ë¸ ì‹œê°í™”
plt.figure(figsize=(20,10))
plot_tree(
    dt_clf,
    filled=True,
    feature_names=iris_data.feature_names,
    class_names=iris_data.target_names
)

plt.show()
```

## DecisionTreeRegressor - íšŒê·€
- ê° ë…¸ë“œì—ì„œ MSEë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë…¸ë“œ ë¶„í• 
    - MSEì˜ ê²½ìš° ëª¨ë¸ì˜ í‰ê°€ë¥¼ ìœ„í•œ ê²€ì¦ì ìˆ˜ë¡œ í™œìš©ë¨
- ìµœì¢… ë…¸ë“œ(ë¦¬í”„ë…¸ë“œ)ì—ì„œëŠ” ê° ìƒ˜í”Œë“¤ì˜ í‰ê· ê°’ì„ ê³„ì‚°í•´ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì‚¬ìš©
```python
from sklearn.datasets import fetch_california_housing
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score

housing_data = fetch_california_housing()
housing_df = pd.DataFrame(housing_data.data, columns=housing_data.feature_names)
housing_df[housing_data.target_names[0]] = housing_data.target
housing_df.info()

#í•™ìŠµ
dt_reg = DecisionTreeRegressor(random_state=0)
dt_reg.fit(X_train,y_train)

# ì˜ˆì¸¡
pred_train = dt_reg.predict(X_train)
pred_test = dt_reg.predict(X_test)

# í‰ê°€
mse_train = mean_squared_error(y_train,pred_train)
r2_train = r2_score(y_train, pred_train)

mse_test = mean_squared_error(y_test,pred_test)
r2_test = r2_score(y_test, pred_test)

print('train ë°ì´í„° í‰ê°€ :', mse_train, '|', r2_train)
print('test ë°ì´í„° í‰ê°€ : ', mse_test, '|', r2_test)

# ì‹œê°í™”
from sklearn.tree import plot_tree

plt.figure(figsize=(20, 10))
plot_tree(
    dt_reg,
    filled=True,
    feature_names=housing_data.feature_names,
    max_depth=3
)
plt.show()
```
---
## SVM(Support Vector Machine)
- ***ì´ì§„ ë¶„ë¥˜*** ë¬¸ì œ í•´ê²° (ë¶„ë¥˜ ëª¨ë¸)
- SVMí˜¸ì¶œí•œ í•¨ìˆ˜ì˜ ì¸ìì— ë‹´ê²¨ì§€ëŠ” **í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ ìš”ì†Œ**ì— ë”°ë¼ ê·œì œë¥¼ ì¤˜ì„œ ì„±ëŠ¥ì— ì˜í–¥ì„ ì¤„ìˆ˜ ìˆë‹¤.
    - C : í•™ìŠµ ë°ì´í„°ì˜ ì˜¤ë¥˜ í—ˆìš©ë„ ê²°ì •
        - ê°’ì˜ í¬ê¸°ì— ë¹„ë¡€í•˜ì—¬ ë§ˆì§„ì˜ ë²”ìœ„ê°€ ë„“ì–´ì§
            - Cì˜ ê°’ ì¦ê°€ -> ê³¼ëŒ€ ì í•© ê°€ëŠ¥ì„± ì¦ê°€
            - Cì˜ ê°’ ê°ì†Œ -> ê³¼ì†Œ ì í•© ê°€ëŠ¥ì„± ì¦ê°€
    - ***Kernel*** : ë¹„ì„ í˜• ë°ì´í„°ì˜ ë³€í™˜ì„ ìœ„í•œ ì»¤ë„ í•¨ìˆ˜ ì„¤ì •
        - linear : ì„ í˜• ì»¤ë„
        - ploy : ë‹¤í•­ì‹ ì»¤ë„ (ë¹„ì„ í˜• ê´€ê³„, ì°¨ìˆ˜ëŠ” degreeë¡œ ì„¤ì • ê°€ëŠ¥)
        - rbf : Radial Basis Function, ê°€ìš°ì‹œì•ˆ ì»¤ë„ ë¹„ì„ í˜• ë°ì´í„° ì²˜ë¦¬
        - sigmoid : ì‹œê·¸ëª¨ì´ë“œ ì»¤ë„
        
```python
# ë°ì´í„° ì¤€ë¹„
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris_data = load_iris()
X_train, X_test, y_train, y_test = \
    train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=42)

# ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
from sklearn.svm import SVC

model = SVC(kernel="linear")
model.fit(X_train, y_train)

model.score(X_test, y_test)

# í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
X = iris_data.data[:, :2]
y = iris_data.target

X_train, X_test, y_train, y_test = \
    train_test_split(X, y, test_size=0.2, random_state=42)

# ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
svm_clf = SVC(kernel="linear", C=1.0)

svm_clf.fit(X_train, y_train)

svm_clf.score(X_train, y_train), svm_clf.score(X_test, y_test)

# ì‹œê°í™”
from sklearn.inspection import DecisionBoundaryDisplay

# ê²°ì • ê²½ê³„
dbd = DecisionBoundaryDisplay.from_estimator(svm_clf, X_train, alpha=0.7)

# í›ˆë ¨ë°ì´í„° ì‚°ì ë„
plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolor="k", label="Trainig Data")

plt.xlabel('sepal length')
plt.ylabel('sepal width')
plt.legend()
plt.show()
```

- ***Decision BoundaryDisplay ë¼ì´ë¸ŒëŸ¬ë¦¬***
    - í´ë˜ìŠ¤ë³„ ëª¨ë¸ì„ ë¶„ë¥˜ì‹œì¼œ ê·¸ê²ƒì„ ì‹œê°í•˜ê¸° ìœ„í•´ ê°’ì„ ë¶„ë¥˜í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
    - ì¸ìë¡œ ì‚¬ìš©í•œ í•™ìŠµ ëª¨ë¸ê³¼, í•™ìŠµ ë°ì´í„°, ê·œì œì •ë„(alpha) ì„¤ì • ê°€ëŠ¥



## SVR(Suppoter Vector Regressor)
- ì—°ì†ì ì¸ ê°’ ì˜ˆì¸¡ (íšŒê·€ ëª¨ë¸)
- SVR ë˜í•œ í˜¸ì¶œí•œ í•¨ìˆ˜ì˜ ì¸ìì— ë‹´ê¸´ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ ìš”ì†Œì— ë”°ë¼ ë°ì´í„° ë³€í™˜ í˜•ì‹ì´ ë‹¤ë¦„
- **ì‘ë™ ì›ë¦¬**

    - 1. **ğœ–-íŠœë¸Œ(ì—¡ì‹¤ë¡  íŠœë¸Œ)**
        - ë°ì´í„° í¬ì¸íŠ¸ì™€ ì˜ˆì¸¡ ê°’ ì‚¬ì´ì˜ í—ˆìš© ì˜¤ì°¨ ë²”ìœ„ë¥¼ ì •ì˜
        - ğœ–(ì—¡ì‹¤ë¡  íŠœë¸Œ) : ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ì°¨ê°€ ë²”ìœ„ ë‚´ì— ìˆìœ¼ë©´ ë¬´ì‹œ, ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ë²Œì¹™(Penalty)ì„ ë¶€ì—¬

    - 2. **ìµœì í™” ëª©í‘œ**
        - ğœ–-íŠœë¸Œ ë‚´ë¶€ì— ë°ì´í„°ë¥¼ í¬í•¨í•˜ë©´ì„œ, ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•˜ê³  ë§ˆì§„(Margin)ì„ ìµœëŒ€í™”

    - 3. **ì»¤ë„ íŠ¸ë¦­**
        - ë¹„ì„ í˜• ë°ì´í„°ë¥¼ ê³ ì°¨ì› ê³µê°„ìœ¼ë¡œ ë§¤í•‘í•˜ì—¬ ì„ í˜•ì ìœ¼ë¡œ í•´ê²° ê°€ëŠ¥

```python
# ë°ì´í„° ì¤€ë¹„
np.random.seed(0)
X = np.sort(np.random.rand(40, 1) * 5, axis=0)
y = np.sin(X).ravel() + np.random.randn(40) * 0.1

X_test = np.linspace(0, 5, 100).reshape(-1, 1)

# ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡
from sklearn.svm import SVR

svr_rbf = SVR(kernel='rbf')
svr_lin = SVR(kernel='linear')
svr_poly = SVR(kernel='poly')

# í•™ìŠµ
svr_rbf.fit(X, y)
svr_lin.fit(X, y)
svr_poly.fit(X, y)

# ì˜ˆì¸¡
rbf_pred = svr_rbf.predict(X_test)
lin_pred = svr_lin.predict(X_test)
poly_pred = svr_poly.predict(X_test)

# ê²°ê³¼ ì‹œê°í™”
plt.scatter(X, y, color='darkorange', label='data')
plt.plot(X_test, rbf_pred, color='navy', label='rbf_pred')
plt.plot(X_test, lin_pred, color='c', label="lin_pred")
plt.plot(X_test, poly_pred, color='cornflowerblue', label='poly_pred')

# epsilon ì˜¤ì°¨ ë²”ìœ„ ì‹œê°í™”
svr_rbf_epsilon = svr_rbf.epsilon
print(svr_rbf_epsilon)
epsilon_upper = rbf_pred + svr_rbf_epsilon
epsilon_lower = rbf_pred - svr_rbf_epsilon
plt.fill_between(X_test.ravel(), epsilon_lower, epsilon_upper, color="skyblue", alpha=0.3)

plt.xlabel('Data')
plt.ylabel('Target')
plt.legend()
plt.show()

```

- Epsilonì˜ í—ˆìš© ë²”ìœ„ë¥¼ ì•Œì•„ë‚´ê²Œ í•´ì£¼ëŠ” ì½”ë“œ
    - 

| íŠ¹ì§•                   | SVM                                    | SVR                                    |
|----------------------|---------------------------------------|---------------------------------------|
| **ëª©ì **              | ì´ì§„ ë¶„ë¥˜ ë¬¸ì œ í•´ê²°                   | ì—°ì†ì ì¸ ê°’ ì˜ˆì¸¡                      |
| **ê²°ì • ê²½ê³„**         | ì„œí¬íŠ¸ ë²¡í„°ì™€ì˜ ê±°ë¦¬ë¥¼ ìµœëŒ€í™”í•˜ì—¬ ìƒì„± | ë°ì´í„° í¬ì¸íŠ¸ì™€ì˜ ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•˜ì—¬ ìƒì„± |
| **ë§ˆì§„/í—ˆìš© ì˜¤ì°¨**    | ë§ˆì§„ì„ ìµœëŒ€í™”í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ    | Îµ ë§¤ê°œë³€ìˆ˜ë¡œ í—ˆìš© ì˜¤ì°¨ ë²”ìœ„ ì„¤ì •       |
| **ê²°ê³¼**              | í´ë˜ìŠ¤ ì˜ˆì¸¡ (ì´ì§„ ë¶„ë¥˜)               | ì—°ì†ì ì¸ ê°’ ì˜ˆì¸¡                      |


---
