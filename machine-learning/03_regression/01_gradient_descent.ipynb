{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법(Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습률(Learning Rate)\n",
    "- 최적의 해를 빠르게 혹은 천천히 조금씩 찾아가는 '정도'를 가르키는 하이퍼 파라미터\n",
    "- 기본값으로 보통 0.001을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 잔차제곱합(Residual Sum of Squares, Rss)\n",
    "- 잔차 = 실제 값 - 예측 값\n",
    "- 잔차제곱합 = (실제 값 - 예측 값)의 제곱의 합\n",
    "- 회긔 모델의 정확도를 측정하는 지표\n",
    "    - 잔차가 작을수록 정확하게 예측하는 모델\n",
    "    - 잔차가 클수록록 잘못된 예측하는 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모든 회귀 모델은 RSS가 최소가 되는 방향으로 학습이 진행됨 = 회귀계수(절편)는 RSS가 최소가 되도록 학습\n",
    "    - 비용 함수 R(w)가 가장 작을 떄의 w를 찾는 것이 회귀 모델의 목표\n",
    "        - 매 회차에 계싼된 R(w)에서 순간변화율(기울기)를 구해야 함 -> 미분 사용\n",
    "        - 단, 우리가 구해야 하는 회귀계쑤는 하나 이상이므로 우리는 편미분을 사용함\n",
    "            - wO(절편)을 고정한 채로 w1의 미분을 구하고, w1을 고정한 채로 w0 미분을 구함함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**경사하강법 공식**\n",
    "\n",
    "$w1 = w1 - (미분값)$\n",
    "\n",
    "$w1 = w1 - (-학습률 * 2 / N * (x * (실제값 - 예측값))의 합)$\n",
    "\n",
    "$w0 = w0 - (미분값)$\n",
    "\n",
    "$w0 = w0 - (-학습률 * 2 / N * (x * (실제값 - 예측값))의 합)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "X = np.random.rand(100, 1)\n",
    "# print(X.shape)\n",
    "# print(X)\n",
    "\n",
    "\n",
    "noise = np.random.rand(100, 1)      # 정규분포 난수를 노이즈로 사용\n",
    "y = 6 + 4 * X + noise               # 절편이 6? X 개수가 4?\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gredient_descent(X,y, learning_rate=0.01, max_iter=1000):\n",
    "    # 경사하강법 실행\n",
    "\n",
    "    # 회귀계수(가중치) 초기화\n",
    "    w0 = np.zeros((1,1))\n",
    "    w1 = np.zeros((1,1))\n",
    "\n",
    "    #학습률\n",
    "    # learning_rate = 0.0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        #잔차 계산\n",
    "        y_pred = w0 + np.dot(X, w1)\n",
    "        diff = y - y_pred\n",
    "\n",
    "        N = len(X)\n",
    "\n",
    "        # w0 편미분 (w0를 갱신할 값)\n",
    "        w0_diff = -learning_rate * 2 / N * np.sum(diff)\n",
    "        # 절편(w0) 갱신\n",
    "        w0 = w0 - w0_diff\n",
    "\n",
    "        # w1 편미분 (w1를 갱신할 값)\n",
    "        # w1 = w1 - (-학습률 * 2 / N * (x * (실제값 - 예측값))의 합)\n",
    "        w1_diff = - learning_rate * 2 / N * np.dot(X.T,diff)\n",
    "        # 가중치(w1) 갱신\n",
    "        w1 = w1- w1_diff\n",
    "\n",
    "        # 시각화\n",
    "        plt.figure(figsize=(3, 2))\n",
    "        plt.scatter(X, y)\n",
    "        plt.plot(X, y_pred, color='red')\n",
    "        plt.show()\n",
    "\n",
    "    return w0, w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(y_true,y_pred):\n",
    "    return np.sum((y_true - y_pred) ** 2) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0,w1 = gredient_descent(X,y)\n",
    "\n",
    "print('최종회구 계수(가중치 w1) : ', w1)\n",
    "print('최종 회귀 계수(절편) : ' , w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = w0 + np.dot(X,w1)\n",
    "print('비용합수 결과 : ' ,cost_function(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,y)\n",
    "plt.plot(X,y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미니 배치 경사 하강법(Mini-batch Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(X,y ,batch_size=10,learning_rate=0.01, max_iter=1000):\n",
    "    # 경사하강법 실행\n",
    "\n",
    "    # 회귀계수(가중치) 초기화\n",
    "    w0 = np.zeros((1,1))\n",
    "    w1 = np.zeros((1,1))\n",
    "\n",
    "    #학습률\n",
    "    # learning_rate = 0.0\n",
    "\n",
    "    #데이터의 개수\n",
    "    N = batch_size\n",
    "    \n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # 미니배치 데이터 선정\n",
    "        random_index = np.random.permutation(X.shape[0])\n",
    "        X_sample = X[random_index[:batch_size]]\n",
    "        y_sample = y[random_index[:batch_size]]\n",
    "    \n",
    "        #잔차 계산\n",
    "        y_pred = w0 + np.dot(X_sample, w1)\n",
    "        diff = y_sample - y_pred\n",
    "\n",
    "        N = len(X)\n",
    "\n",
    "        # w0 편미분 (w0를 갱신할 값)\n",
    "        w0_diff = -learning_rate * 2 / N * np.sum(diff)\n",
    "        # 절편(w0) 갱신\n",
    "        w0 = w0 - w0_diff\n",
    "\n",
    "        # w1 편미분 (w1를 갱신할 값)\n",
    "        # w1 = w1 - (-학습률 * 2 / N * (x * (실제값 - 예측값))의 합)\n",
    "        w1_diff = - learning_rate * 2 / N * np.dot(X_sample.T,diff)\n",
    "        # 가중치(w1) 갱신\n",
    "        w1 = w1- w1_diff\n",
    "\n",
    "        # 시각화\n",
    "        plt.figure(figsize=(3, 2))\n",
    "        plt.scatter(X, y)\n",
    "        plt.plot(X_sample, y_pred, color='red')\n",
    "        plt.show()\n",
    "\n",
    "    return w0, w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0,w1 = mini_batch_gradient_descent(X,y)\n",
    "\n",
    "print('최종회구 계수(가중치 w1) : ', w1) #model.coef_\n",
    "print('최종 회귀 계수(절편) : ' , w0)    #model.intercept_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
