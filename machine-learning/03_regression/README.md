# 경사하강법 (Gradient Descent)
## 학습률 (Learning Rate)
- 최적의 해를 빠르게 혹은 천천히 조금씩 찾아가는 '정도'를 가르키는 하이퍼 파라미터
- 기본 값으로 보통 0.001 사용

## 잔차제곱합(Residual Sum of Squares ,RSS)
- 잔차 = 실제 값 - 예측값
- 잔차제곱합 = (실제값 - 예측값) 의 제곱합
- 회귀 모델의 정확도를 측정하는 지표
  - RSS가 작을수록 정확한 모델
  - RSS가 클수록 잘못된 예측 모델
- 모든 회귀 모델은 RSS가 최소가 되는 방향으로 학습 진행 = 회귀계수(=절편)는 RSS가 최소가 되도록 학습
- 비용함수 R(w)가 가장 작을 때의 w를 찾는 것이 회귀 모델의 목표
  - 매 회차에 계산된 R(w)에서 순간변화율(기울기) 구해야함 -> 미분 사용
  - 우리가 구해야 하는 회귀계수는 하나 이상이므로 우리는 ***편미분*** 사용
    - wO(절편) 고정한 채로 w1의 미분을 구하고, w1을 고정한 채로 wO 미분을 구함
    - 
**경사하강법 수식**
$w_1$ $w_0$을 반복적으로 업데이트하며 최적의 회귀계수를 찾음
<br/>
$w_1 = w_1 - (-η\frac{2}{N}\sum^{N}_{i=1} x_i * (실제값_i - 예측값_i))$
<br/>
$w_0 = w_0 - (-η\frac{2}{N}\sum^{N}_{i=1}(실제값_i - 예측값_i))$

**경사하강법 공식**

$w1 = w1 - (미분값)$

$w1 = w1 - (-학습률 * 2 / N * (x * (실제값 - 예측값))의 합)$

$w0 = w0 - (미분값)$

$w0 = w0 - (-학습률 * 2 / N * (실제값 - 예측값)의 합)$

### 경사하강법 실행 예시
- w0 : 절편(=bias), w1: 기울기(=weights) 초기화
```python
w0 = np.zeros((1, 1))
w1 = np.zeros((1, 1))
```
- 예측값(pred) 계산
```python
# 내적 연산함수(dot())를 통해 X와 w1의 각각의 행열을 내적 연산한다.
y_pred = w0 + np.dot(X, w1)
```
- 잔차(diff) 계산
```python
# 현재값 - 예측값
diff = y - y_pred
```
- 학습률(learning_rate) 설정
```python
learning_rate = 0.1
```
- 데이터 개수 파악
```python
# 평균 오차 계산때 사용
N = len(X)
```
- 가중치(절편 + 기울기) 업데이트
```python
# w0 편미분 (w0를 갱신할 값)
# w0 = w0 - (-학습률 * 2 / N * (실제값 - 예측값)의 합)
w0_diff = -learning_rate * 2 / N * np.sum(diff)
# 절편(w0) 갱신
w0 = w0 - w0_diff

# w1 편미분 (w1를 갱신할 값)
# w1 = w1 - (-학습률 * 2 / N * (x * (실제값 - 예측값))의 합)
w1_diff = -learning_rate * 2 / N * np.dot(X.T, diff)    # (100, 1) (100, 1)
# 가중치(w1) 갱신
w1 = w1 - w1_diff

print(f'1회 업데이트된 회귀계수 w0: {w0}, w1: {w1} ')
```